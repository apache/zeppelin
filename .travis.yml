# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

language: java

before_cache:
    - sudo chown -R travis:travis $HOME/.m2
    # Ensure that jobs do not influence each other with installed Zeppelin Jars
    - rm -rf $HOME/.m2/repository/org/apache/zeppelin/

cache:
  apt: true
  directories:
    - ${HOME}/.m2
    -  zeppelin-web/node
    -  zeppelin-web/node_modules

env:
  global:
    # Interpreters does not required by zeppelin-server integration tests
    - INTERPRETERS='!beam,!hbase,!pig,!jdbc,!file,!flink,!ignite,!kylin,!lens,!cassandra,!elasticsearch,!bigquery,!alluxio,!scio,!livy,!groovy,!sap,!java,!geode,!neo4j,!hazelcastjet,!submarine,!sparql,!mongodb'
    - CI="true"

jobs:
  include:
    - name: "Test License compliance using RAT tool"
      jdk: "openjdk8"
      dist: xenial
      env: CACHE_NAME=rat
      before_install:
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit'" >> ~/.mavenrc
      install:
        - mvn clean -Prat -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn org.apache.rat:apache-rat-plugin:check -Prat -B


    - name: "Default build command, no tests"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
      install:
        - mvn clean package -T C2 -DskipTests -Pweb-angular -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipTests -Pweb-angular -B


    - name: "Run e2e tests in zeppelin-web"
      os: linux
      dist: xenial
      jdk: "openjdk8"
      services:
        - xvfb
      addons:
        apt:
          packages:
          - google-chrome-stable
      before_install:
        - export PYTHON=2
        - export SPARK_VER="2.1.0"
        - export HADOOP_VER="2.6"
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl ${INTERPRETERS} -Phadoop2 -Pscala-2.11 -B
      before_script:
        - travis_retry ./testing/downloadSpark.sh $SPARK_VER $HADOOP_VER
        - export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER
        - echo "export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER" > conf/zeppelin-env.sh
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn verify -DskipRat -pl zeppelin-web -Phadoop2 -Pscala-2.11 -B -Pweb-e2e


    - name: "Run tests in zeppelin-web-angular"
      os: linux
      dist: xenial
      jdk: "openjdk8"
      services:
        - xvfb
      addons:
        apt:
          packages:
            - google-chrome-stable
      before_install:
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
      install:
        - mvn clean -DskipTests -DskipRat -pl ${INTERPRETERS} -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn package -DskipRat -pl zeppelin-web-angular -Pweb-angular -B


    # After issues are fixed tests needs to be included back by removing them from the "-Dtests.to.exclude" property
    - name: "Test core modules (zeppelin-interpreter,zeppelin-zengine,zeppelin-server) on hadoop2"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -Pbuild-distr -DskipRat -DskipTests -pl zeppelin-server,zeppelin-web,spark/spark-dependencies,markdown,angular,shell -am -Phelium-dev -Pexamples -Phadoop2 -B
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn verify -Pusing-packaged-distr -DskipRat -pl zeppelin-server,zeppelin-web,spark/spark-dependencies,markdown,angular,shell -am -Phelium-dev -Pexamples -Phadoop2 -B -Dtests.to.exclude=**/org/apache/zeppelin/spark/* -DfailIfNoTests=false


    - name: "Test core modules (zeppelin-interpreter,zeppelin-zengine,zeppelin-server) on hadoop3"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -Pbuild-distr -DskipRat -DskipTests -pl zeppelin-server,zeppelin-web,spark/spark-dependencies,markdown,angular,shell -am -Phelium-dev -Pexamples -Phadoop3 -B
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn verify -Pusing-packaged-distr -DskipRat -pl zeppelin-server,zeppelin-web,spark/spark-dependencies,markdown,angular,shell -am -Phelium-dev -Pexamples -Phadoop3 -B -Dtests.to.exclude=**/org/apache/zeppelin/spark/* -DfailIfNoTests=false


    - name: "Test selenium with spark module for spark 2.3"
      jdk: "openjdk8"
      dist: xenial
      addons:
        firefox: "31.0"
      before_install:
        - export PYTHON=2
        - export R=true
        - export SPARK_VER="2.3.2"
        - export HADOOP_VER="2.6"
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - "/sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -ac -screen 0 1600x1024x16"
        - source ~/.environ
      install:
        - mvn clean install -DskipTests -DskipRat -pl ${INTERPRETERS} -Pspark-2.3 -Phadoop2 -Phelium-dev -Pexamples -Pintegration -Pspark-scala-2.11 -B
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - travis_retry ./testing/downloadSpark.sh $SPARK_VER $HADOOP_VER
        - export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER
        - echo "export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER" > conf/zeppelin-env.sh
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn verify -DskipRat -Pspark-2.3 -Phadoop2 -Phelium-dev -Pexamples -Pintegration -Pspark-scala-2.11 -B -pl zeppelin-integration -DfailIfNoTests=false


    - name: "Test interpreter modules"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export TENSORFLOW=1.13.1
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -am -pl $(echo .,zeppelin-interpreter,zeppelin-interpreter-shaded,${INTERPRETERS} | sed 's/!//g') -Pscala-2.10 -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl $(echo .,zeppelin-interpreter,zeppelin-interpreter-shaded,${INTERPRETERS} | sed 's/!//g') -Pscala-2.10 -B


    - name: "Test zeppelin-client integration test"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export PYTHON=3
        - export R=true
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -Pintegration -pl zeppelin-interpreter-integration,zeppelin-web,spark/spark-dependencies,markdown,flink/interpreter,jdbc,shell -am
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl zeppelin-interpreter-integration -Pintegration -Dtest=ZeppelinClientIntegrationTest,ZeppelinClientWithAuthIntegrationTest,ZSessionIntegrationTest


    - name: "Test flink 1.10 & flink integration test"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export FLINK=1.10.1
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -am -pl flink/interpreter,zeppelin-interpreter-integration -Pflink-1.10 -Pintegration -B
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl flink/interpreter,zeppelin-interpreter-integration -Pflink-1.10 -Pintegration -B -Dtest=org.apache.zeppelin.flink.*,FlinkIntegrationTest110,ZeppelinFlinkClusterTest110


    - name: "Test flink 1.11 & flink integration test"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export FLINK=1.11.1
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -am -pl flink/interpreter,zeppelin-interpreter-integration -Pflink-1.11 -Pintegration -B
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl flink/interpreter,zeppelin-interpreter-integration -Pflink-1.11 -Pintegration -B -Dtest=org.apache.zeppelin.flink.*,FlinkIntegrationTest111,ZeppelinFlinkClusterTest111


    # Run Spark integration test and unit test
    - name: "Run spark integration of in one zeppelin instance: Spark 3.0"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl zeppelin-interpreter-integration,zeppelin-web,spark/spark-dependencies,markdown -am -Phadoop2 -Pintegration -B
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl zeppelin-interpreter-integration,zeppelin-web,spark/spark-dependencies,markdown -am -Phadoop2 -Pintegration -B -Dtest=ZeppelinSparkClusterTest30,SparkIntegrationTest30 -DfailIfNoTests=false


    - name: "Run spark integration of in one zeppelin instance (2.4, 2.3, 2.2)"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl zeppelin-interpreter-integration,zeppelin-web,spark/spark-dependencies,markdown -am -Phadoop2 -Pintegration -B
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl zeppelin-interpreter-integration,zeppelin-web,spark/spark-dependencies,markdown -am -Phadoop2 -Pintegration -B -Dtest=ZeppelinSparkClusterTest24,SparkIntegrationTest24,ZeppelinSparkClusterTest23,SparkIntegrationTest23,ZeppelinSparkClusterTest22,SparkIntegrationTest22 -DfailIfNoTests=false


    - name: "JdbcIntegrationTest, Unit test of Spark 2.4 (Scala-2.11)"
      jdk: "openjdk8"
      dist: xenial
      services:
        - mysql
      before_install:
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl zeppelin-interpreter-integration,jdbc,zeppelin-web,spark/spark-dependencies,markdown -am -Pspark-2.4 -Pspark-scala-2.11 -Phadoop2 -Pintegration -B
        - mvn clean package -T 2C -pl zeppelin-plugins -amd -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl zeppelin-interpreter-integration,jdbc,zeppelin-web,spark/spark-dependencies,markdown -am -Pspark-2.4 -Pspark-scala-2.11 -Phadoop2 -Pintegration -B -Dtest=JdbcIntegrationTest,org.apache.zeppelin.spark.*,org.apache.zeppelin.kotlin.* -DfailIfNoTests=false


    - name: "Unit test of Spark 2.4 (Scala-2.12)"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl spark/spark-dependencies -am -Pspark-2.4 -Pspark-scala-2.12 -Phadoop2 -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl spark/spark-dependencies -am -Pspark-2.4 -Pspark-scala-2.12 -Phadoop2 -B -Dtest=org.apache.zeppelin.spark.*,org.apache.zeppelin.kotlin.* -DfailIfNoTests=false


    - name: "Unit test of Spark 2.3 (Scala-2.11) and Unit test python, jupyter and r interpreter under python3"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl spark/spark-dependencies -am -Pspark-2.3 -Pspark-scala-2.11 -Phadoop2 -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl spark/spark-dependencies -am -Pspark-2.3 -Pspark-scala-2.11 -Phadoop2 -B -Dtest=org.apache.zeppelin.spark.*,apache.zeppelin.python.*,apache.zeppelin.jupyter.*,apache.zeppelin.r.* -DfailIfNoTests=false


    - name: "Unit test of Spark 2.2 (Scala-2.10) and Unit test python, jupyter and r interpreter under python3"
      jdk: "openjdk8"
      dist: xenial
      before_install:
        - export R=true
        - export PYTHON=3
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl spark/spark-dependencies -am -Pspark-2.2 -Pspark-scala-2.10 -Phadoop2 -B
      before_script:
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn test -DskipRat -pl spark/spark-dependencies -am -Pspark-2.2 -Pspark-scala-2.10 -Phadoop2 -B -Dtest=org.apache.zeppelin.spark.*,apache.zeppelin.python.*,apache.zeppelin.jupyter.*,apache.zeppelin.r.* -DfailIfNoTests=false


    - name: "Test python/pyspark with python 2, livy 0.5"
      dist: xenial
      jdk: "openjdk8"
      before_install:
        - export PYTHON="2"
        - export SPARK_VER="1.6.3"
        - export HADOOP_VER="2.6"
        - export LIVY_VER="0.5.0-incubating"
        - export R="true"
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl livy -am -Pspark-1.6 -Phadoop2 -Pscala-2.10 -B
      before_script:
        - travis_retry ./testing/downloadSpark.sh $SPARK_VER $HADOOP_VER
        - ./testing/downloadLivy.sh $LIVY_VER
        - export LIVY_HOME=`pwd`/livy-$LIVY_VER-bin
        - export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER
        - export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER
        - echo "export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER" > conf/zeppelin-env.sh
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn verify -DskipRat -pl livy -am -Pspark-1.6 -Phadoop2 -Pscala-2.10 -B


    - name: "Test livy 0.5 with spark 2.2.0 under python3"
      dist: xenial
      jdk: "openjdk8"
      before_install:
        - export PYTHON="3"
        - export SPARK_VER="2.2.0"
        - export HADOOP_VER="2.6"
        - export LIVY_VER="0.5.0-incubating"
        - export R="true"
        - echo "MAVEN_OPTS='-Xms1024M -Xmx2048M -XX:MaxMetaspaceSize=1024m -XX:-UseGCOverheadLimit -Dorg.slf4j.simpleLogger.defaultLogLevel=warn'" >> ~/.mavenrc
        - bash -x ./testing/install_external_dependencies.sh
        - source ~/.environ
      install:
        - mvn install -DskipTests -DskipRat -pl livy -am  -B
      before_script:
        - travis_retry ./testing/downloadSpark.sh $SPARK_VER $HADOOP_VER
        - ./testing/downloadLivy.sh $LIVY_VER
        - export LIVY_HOME=`pwd`/livy-$LIVY_VER-bin
        - export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER
        - export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER
        - echo "export SPARK_HOME=`pwd`/spark-$SPARK_VER-bin-hadoop$HADOOP_VER" > conf/zeppelin-env.sh
        - echo "export ZEPPELIN_HELIUM_REGISTRY=helium" >> conf/zeppelin-env.sh
        - echo "export SPARK_PRINT_LAUNCH_COMMAND=true" >> conf/zeppelin-env.sh
        - export SPARK_PRINT_LAUNCH_COMMAND=true
        - tail conf/zeppelin-env.sh
      script:
        - mvn verify -DskipRat -pl livy -am  -B


after_success:
  - echo "Travis exited with ${TRAVIS_TEST_RESULT}"
  - cat logs/zeppelin-interpreter-flink-*.log

after_failure:
  - echo "Travis exited with ${TRAVIS_TEST_RESULT}"
  - find . -name rat.txt | xargs cat
  - cat logs/*
  - cat zeppelin-distribution/target/zeppelin-*-SNAPSHOT/zeppelin-*-SNAPSHOT/logs/zeppelin*.log
  - cat zeppelin-distribution/target/zeppelin-*-SNAPSHOT/zeppelin-*-SNAPSHOT/logs/zeppelin*.out
  - cat zeppelin-web/npm-debug.log
  - cat spark-*/logs/*
  - cat livy/target/tmp/*/output.log
  - cat livy/target/tmp/livy-int-test/*/output.log
  - ls -R livy/target/tmp/livy-int-test/MiniYarnMain/target/org.apache.livy.test.framework.MiniYarnMain/*
  - cat livy/target/tmp/livy-int-test/MiniYarnMain/target/org.apache.livy.test.framework.MiniYarnMain/*/*/*/stdout
  - cat livy/target/tmp/livy-int-test/MiniYarnMain/target/org.apache.livy.test.framework.MiniYarnMain/*/*/*/stderr
  - cat zeppelin-zengine/target/org.apache.zeppelin.interpreter.MiniHadoopCluster/*/*/*/stdout
  - cat logs/zeppelin-interpreter-flink-*.log
