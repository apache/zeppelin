<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one or more
  ~ contributor license agreements.  See the NOTICE file distributed with
  ~ this work for additional information regarding copyright ownership.
  ~ The ASF licenses this file to You under the Apache License, Version 2.0
  ~ (the "License"); you may not use this file except in compliance with
  ~ the License.  You may obtain a copy of the License at
  ~
  ~    http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <parent>
    <artifactId>zeppelin</artifactId>
    <groupId>org.apache.zeppelin</groupId>
    <version>0.8.0-SNAPSHOT</version>
    <relativePath>..</relativePath>
  </parent>

  <groupId>org.apache.zeppelin</groupId>
  <artifactId>zeppelin-spark-dependencies_2.10</artifactId>
  <packaging>jar</packaging>
  <version>0.8.0-SNAPSHOT</version>
  <name>Zeppelin: Spark dependencies</name>
  <description>Zeppelin spark support</description>

  <properties>
    <!-- library version defined in this section brought from spark 1.4.1 and it's dependency.
         Therefore changing only spark.version is not going to be enough when this module
         support new version of spark to make the new version as default supported version.

         Each profile (spark-2.0, spark-1.6, etc) will overrides necessary dependency version.
         So we'll make one of those profile 'activateByDefault' to make it default supported version
         instead of changing spark.version in this section.
    -->

    <spark.version>1.4.1</spark.version>
    <hadoop.version>2.3.0</hadoop.version>
    <yarn.version>${hadoop.version}</yarn.version>
    <avro.version>1.7.7</avro.version>
    <avro.mapred.classifier/>
    <jets3t.version>0.7.1</jets3t.version>
    <protobuf.version>2.4.1</protobuf.version>

    <akka.group>org.spark-project.akka</akka.group>
    <akka.version>2.3.4-spark</akka.version>

    <spark.archive>spark-${spark.version}</spark.archive>
    <spark.src.download.url>
      http://d3kbcqa49mib13.cloudfront.net/${spark.archive}.tgz
    </spark.src.download.url>
    <spark.bin.download.url>
      http://d3kbcqa49mib13.cloudfront.net/${spark.archive}-bin-without-hadoop.tgz
    </spark.bin.download.url>
    <spark.py4j.version>0.8.2.1</spark.py4j.version>

    <!--plugin versions-->
    <plugin.shade.version>2.3</plugin.shade.version>
  </properties>

  <dependencyManagement>
    <dependencies>
      <dependency>
        <groupId>org.apache.avro</groupId>
        <artifactId>avro</artifactId>
        <version>${avro.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.avro</groupId>
        <artifactId>avro-ipc</artifactId>
        <version>${avro.version}</version>
        <exclusions>
          <exclusion>
            <groupId>io.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>jetty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>jetty-util</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>servlet-api</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.apache.velocity</groupId>
            <artifactId>velocity</artifactId>
          </exclusion>
        </exclusions>
      </dependency>
      <dependency>
        <groupId>org.apache.avro</groupId>
        <artifactId>avro-mapred</artifactId>
        <version>${avro.version}</version>
        <classifier>${avro.mapred.classifier}</classifier>
        <exclusions>
          <exclusion>
            <groupId>io.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>jetty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>jetty-util</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>servlet-api</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.apache.velocity</groupId>
            <artifactId>velocity</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <!-- See SPARK-1556 for info on this dependency: -->
      <dependency>
        <groupId>net.java.dev.jets3t</groupId>
        <artifactId>jets3t</artifactId>
        <version>${jets3t.version}</version>
        <scope>runtime</scope>
        <exclusions>
          <exclusion>
            <groupId>commons-logging</groupId>
            <artifactId>commons-logging</artifactId>
          </exclusion>
        </exclusions>
      </dependency>
      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-yarn-api</artifactId>
        <version>${yarn.version}</version>
        <exclusions>
          <exclusion>
            <groupId>asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.ow2.asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.jboss.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>commons-logging</groupId>
            <artifactId>commons-logging</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-yarn-common</artifactId>
        <version>${yarn.version}</version>
        <exclusions>
          <exclusion>
            <groupId>asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.ow2.asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.jboss.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
          </exclusion>
          <exclusion>
            <groupId>commons-logging</groupId>
            <artifactId>commons-logging</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-yarn-server-web-proxy</artifactId>
        <version>${yarn.version}</version>
        <exclusions>
          <exclusion>
            <groupId>asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.ow2.asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.jboss.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
          </exclusion>
          <exclusion>
            <groupId>commons-logging</groupId>
            <artifactId>commons-logging</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-yarn-client</artifactId>
        <version>${yarn.version}</version>
        <exclusions>
          <exclusion>
            <groupId>asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.ow2.asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.jboss.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
          </exclusion>
          <exclusion>
            <groupId>commons-logging</groupId>
            <artifactId>commons-logging</artifactId>
          </exclusion>
        </exclusions>
      </dependency>
    </dependencies>
  </dependencyManagement>

  <dependencies>
    <!-- Spark -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
      <exclusions>
        <exclusion>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-client</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-repl_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <!-- hadoop -->
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>${hadoop.version}</version>
    </dependency>


    <dependency>
      <groupId>com.google.protobuf</groupId>
      <artifactId>protobuf-java</artifactId>
      <version>${protobuf.version}</version>
    </dependency>

    <dependency>
      <groupId>${akka.group}</groupId>
      <artifactId>akka-actor_${scala.binary.version}</artifactId>
      <version>${akka.version}</version>
    </dependency>
    <dependency>
      <groupId>${akka.group}</groupId>
      <artifactId>akka-remote_${scala.binary.version}</artifactId>
      <version>${akka.version}</version>
    </dependency>
    <dependency>
      <groupId>${akka.group}</groupId>
      <artifactId>akka-slf4j_${scala.binary.version}</artifactId>
      <version>${akka.version}</version>
    </dependency>
    <dependency>
      <groupId>${akka.group}</groupId>
      <artifactId>akka-testkit_${scala.binary.version}</artifactId>
      <version>${akka.version}</version>
    </dependency>
    <dependency>
      <groupId>${akka.group}</groupId>
      <artifactId>akka-zeromq_${scala.binary.version}</artifactId>
      <version>${akka.version}</version>
      <exclusions>
        <exclusion>
          <groupId>${akka.group}</groupId>
          <artifactId>akka-actor_${scala.binary.version}</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

    <!-- yarn (not supported for Spark v1.5.0 or higher) -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-yarn_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-yarn-api</artifactId>
      <version>${yarn.version}</version>
    </dependency>

  </dependencies>

  <profiles>
    <profile>
      <id>spark-1.1</id>
      <dependencies>

      </dependencies>
      <properties>
        <spark.version>1.1.1</spark.version>
        <akka.version>2.2.3-shaded-protobuf</akka.version>
      </properties>
    </profile>

    <profile>
      <id>cassandra-spark-1.1</id>
      <dependencies>
        <dependency>
          <groupId>com.datastax.spark</groupId>
          <artifactId>spark-cassandra-connector_${scala.binary.version}</artifactId>
          <version>1.1.1</version>
          <exclusions>
            <exclusion>
              <groupId>org.joda</groupId>
              <artifactId>joda-convert</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
      </dependencies>
      <properties>
        <spark.version>1.1.1</spark.version>
        <akka.version>2.2.3-shaded-protobuf</akka.version>
      </properties>
    </profile>

    <profile>
      <id>spark-1.2</id>
      <dependencies>
      </dependencies>
      <properties>
        <spark.version>1.2.1</spark.version>
      </properties>
    </profile>

    <profile>
      <id>cassandra-spark-1.2</id>
      <properties>
        <spark.version>1.2.1</spark.version>
      </properties>
      <dependencies>
        <dependency>
          <groupId>com.datastax.spark</groupId>
          <artifactId>spark-cassandra-connector_${scala.binary.version}</artifactId>
          <version>1.2.1</version>
          <exclusions>
            <exclusion>
              <groupId>org.joda</groupId>
              <artifactId>joda-convert</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
      </dependencies>
    </profile>

    <profile>
      <id>spark-1.3</id>

      <properties>
        <spark.version>1.3.1</spark.version>
      </properties>

      <dependencies>
      </dependencies>

    </profile>

    <profile>
      <id>cassandra-spark-1.3</id>
      <properties>
        <spark.version>1.3.0</spark.version>
      </properties>

      <dependencies>
        <dependency>
          <groupId>com.datastax.spark</groupId>
          <artifactId>spark-cassandra-connector_${scala.binary.version}</artifactId>
          <version>1.3.1</version>
          <exclusions>
            <exclusion>
              <groupId>org.joda</groupId>
              <artifactId>joda-convert</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
      </dependencies>
    </profile>

    <profile>
      <id>spark-1.4</id>
      <properties>
        <spark.version>1.4.1</spark.version>
      </properties>

      <dependencies>
      </dependencies>
    </profile>

    <profile>
      <id>cassandra-spark-1.4</id>
      <properties>
        <spark.version>1.4.1</spark.version>
      </properties>

      <dependencies>
        <dependency>
          <groupId>com.datastax.spark</groupId>
          <artifactId>spark-cassandra-connector_${scala.binary.version}</artifactId>
          <version>1.4.0</version>
          <exclusions>
            <exclusion>
              <groupId>org.joda</groupId>
              <artifactId>joda-convert</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
      </dependencies>
    </profile>

    <profile>
      <id>spark-1.5</id>
      <properties>
        <spark.version>1.5.2</spark.version>
        <akka.group>com.typesafe.akka</akka.group>
        <akka.version>2.3.11</akka.version>
        <protobuf.version>2.5.0</protobuf.version>
      </properties>

      <dependencies>
      </dependencies>
    </profile>

    <profile>
      <id>cassandra-spark-1.5</id>
      <properties>
        <spark.version>1.5.1</spark.version>
        <akka.group>com.typesafe.akka</akka.group>
        <akka.version>2.3.11</akka.version>
        <protobuf.version>2.5.0</protobuf.version>
        <guava.version>16.0.1</guava.version>
      </properties>

      <dependencies>
        <dependency>
          <groupId>com.datastax.spark</groupId>
          <artifactId>spark-cassandra-connector_${scala.binary.version}</artifactId>
          <version>1.5.0</version>
          <exclusions>
            <exclusion>
              <groupId>org.joda</groupId>
              <artifactId>joda-convert</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
      </dependencies>
    </profile>

    <profile>
      <id>spark-1.6</id>
      <properties>
        <spark.version>1.6.3</spark.version>
        <spark.py4j.version>0.9</spark.py4j.version>
        <akka.group>com.typesafe.akka</akka.group>
        <akka.version>2.3.11</akka.version>
        <protobuf.version>2.5.0</protobuf.version>
      </properties>
    </profile>

    <profile>
      <id>spark-2.0</id>
      <properties>
        <spark.version>2.0.2</spark.version>
        <protobuf.version>2.5.0</protobuf.version>
        <spark.py4j.version>0.10.3</spark.py4j.version>
      </properties>
    </profile>

    <profile>
      <id>spark-2.1</id>
      <properties>
        <spark.version>2.1.0</spark.version>
        <protobuf.version>2.5.0</protobuf.version>
        <spark.py4j.version>0.10.4</spark.py4j.version>
        <scala.version>2.11.8</scala.version>
      </properties>
    </profile>

    <profile>
      <id>spark-2.2</id>
      <activation>
        <activeByDefault>true</activeByDefault>
      </activation>
      <properties>
        <spark.version>2.2.0</spark.version>
        <protobuf.version>2.5.0</protobuf.version>
        <spark.py4j.version>0.10.4</spark.py4j.version>
      </properties>
    </profile>

    <profile>
      <id>hadoop-0.23</id>
      <!-- SPARK-1121: Adds an explicit dependency on Avro to work around a
        Hadoop 0.23.X issue -->
      <dependencies>
        <dependency>
          <groupId>org.apache.avro</groupId>
          <artifactId>avro</artifactId>
        </dependency>
      </dependencies>
      <properties>
        <hadoop.version>0.23.10</hadoop.version>
      </properties>
    </profile>

    <profile>
      <id>hadoop-1</id>
      <properties>
        <hadoop.version>1.0.4</hadoop.version>
        <avro.mapred.classifier>hadoop1</avro.mapred.classifier>
        <codehaus.jackson.version>1.8.8</codehaus.jackson.version>
        <akka.group>org.spark-project.akka</akka.group>
      </properties>
    </profile>

    <profile>
      <id>hadoop-2.2</id>
      <properties>
        <hadoop.version>2.2.0</hadoop.version>
        <protobuf.version>2.5.0</protobuf.version>
        <avro.mapred.classifier>hadoop2</avro.mapred.classifier>
      </properties>
    </profile>

    <profile>
      <id>hadoop-2.3</id>
      <properties>
        <hadoop.version>2.3.0</hadoop.version>
        <protobuf.version>2.5.0</protobuf.version>
        <jets3t.version>0.9.3</jets3t.version>
        <avro.mapred.classifier>hadoop2</avro.mapred.classifier>
      </properties>
    </profile>

    <profile>
      <id>hadoop-2.4</id>
      <properties>
        <hadoop.version>2.4.0</hadoop.version>
        <protobuf.version>2.5.0</protobuf.version>
        <jets3t.version>0.9.3</jets3t.version>
        <avro.mapred.classifier>hadoop2</avro.mapred.classifier>
      </properties>
    </profile>

    <profile>
      <id>hadoop-2.6</id>
      <properties>
        <hadoop.version>2.6.0</hadoop.version>
        <protobuf.version>2.5.0</protobuf.version>
        <jets3t.version>0.9.3</jets3t.version>
        <avro.mapred.classifier>hadoop2</avro.mapred.classifier>
      </properties>
    </profile>

    <profile>
      <id>hadoop-2.7</id>
      <properties>
        <hadoop.version>2.7.2</hadoop.version>
        <protobuf.version>2.5.0</protobuf.version>
        <jets3t.version>0.9.0</jets3t.version>
        <avro.mapred.classifier>hadoop2</avro.mapred.classifier>
      </properties>
    </profile>

    <profile>
      <id>mapr3</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <hadoop.version>1.0.3-mapr-3.0.3</hadoop.version>
        <yarn.version>2.3.0-mapr-4.0.0-FCS</yarn.version>
        <jets3t.version>0.7.1</jets3t.version>
      </properties>
      <repositories>
        <repository>
          <id>mapr-releases</id>
          <url>http://repository.mapr.com/maven/</url>
          <snapshots>
            <enabled>false</enabled>
          </snapshots>
          <releases>
            <enabled>true</enabled>
          </releases>
        </repository>
      </repositories>
    </profile>

    <profile>
      <id>mapr40</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <hadoop.version>2.4.1-mapr-1503</hadoop.version>
        <yarn.version>2.4.1-mapr-1503</yarn.version>
        <jets3t.version>0.9.3</jets3t.version>
      </properties>
      <dependencies>
        <dependency>
          <groupId>org.apache.curator</groupId>
          <artifactId>curator-recipes</artifactId>
          <version>2.4.0</version>
          <exclusions>
            <exclusion>
              <groupId>org.apache.zookeeper</groupId>
              <artifactId>zookeeper</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
        <dependency>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
          <version>3.4.5-mapr-1503</version>
        </dependency>
      </dependencies>
      <repositories>
        <repository>
          <id>mapr-releases</id>
          <url>http://repository.mapr.com/maven/</url>
          <snapshots>
            <enabled>false</enabled>
          </snapshots>
          <releases>
            <enabled>true</enabled>
          </releases>
        </repository>
      </repositories>
    </profile>

    <profile>
      <id>mapr41</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <hadoop.version>2.5.1-mapr-1503</hadoop.version>
        <yarn.version>2.5.1-mapr-1503</yarn.version>
        <jets3t.version>0.7.1</jets3t.version>
      </properties>
      <dependencies>
        <dependency>
          <groupId>org.apache.curator</groupId>
          <artifactId>curator-recipes</artifactId>
          <version>2.4.0</version>
          <exclusions>
            <exclusion>
              <groupId>org.apache.zookeeper</groupId>
              <artifactId>zookeeper</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
        <dependency>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
          <version>3.4.5-mapr-1503</version>
        </dependency>
      </dependencies>
      <repositories>
        <repository>
          <id>mapr-releases</id>
          <url>http://repository.mapr.com/maven/</url>
          <snapshots>
            <enabled>false</enabled>
          </snapshots>
          <releases>
            <enabled>true</enabled>
          </releases>
        </repository>
      </repositories>
    </profile>

    <profile>
      <id>mapr50</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <hadoop.version>2.7.0-mapr-1506</hadoop.version>
        <yarn.version>2.7.0-mapr-1506</yarn.version>
        <jets3t.version>0.9.3</jets3t.version>
      </properties>
      <dependencies>
        <dependency>
          <groupId>org.apache.curator</groupId>
          <artifactId>curator-recipes</artifactId>
          <version>2.4.0</version>
          <exclusions>
            <exclusion>
              <groupId>org.apache.zookeeper</groupId>
              <artifactId>zookeeper</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
        <dependency>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
          <version>3.4.5-mapr-1503</version>
        </dependency>
      </dependencies>
      <repositories>
        <repository>
          <id>mapr-releases</id>
          <url>http://repository.mapr.com/maven/</url>
          <snapshots>
            <enabled>false</enabled>
          </snapshots>
          <releases>
            <enabled>true</enabled>
          </releases>
        </repository>
      </repositories>
    </profile>

    <profile>
      <id>mapr51</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <hadoop.version>2.7.0-mapr-1602</hadoop.version>
        <yarn.version>2.7.0-mapr-1602</yarn.version>
        <jets3t.version>0.9.3</jets3t.version>
      </properties>
      <dependencies>
        <dependency>
          <groupId>org.apache.curator</groupId>
          <artifactId>curator-recipes</artifactId>
          <version>2.4.0</version>
          <exclusions>
            <exclusion>
              <groupId>org.apache.zookeeper</groupId>
              <artifactId>zookeeper</artifactId>
            </exclusion>
          </exclusions>
        </dependency>
        <dependency>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
          <version>3.4.5-mapr-1503</version>
        </dependency>
      </dependencies>
      <repositories>
        <repository>
          <id>mapr-releases</id>
          <url>http://repository.mapr.com/maven/</url>
          <snapshots>
            <enabled>false</enabled>
          </snapshots>
          <releases>
            <enabled>true</enabled>
          </releases>
        </repository>
      </repositories>
    </profile>

  </profiles>

  <build>
    <plugins>
      <plugin>
        <artifactId>maven-enforcer-plugin</artifactId>
        <executions>
          <execution>
            <id>enforce</id>
            <phase>none</phase>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-surefire-plugin</artifactId>
        <configuration>
          <forkCount>1</forkCount>
          <reuseForks>false</reuseForks>
          <argLine>-Xmx1024m -XX:MaxPermSize=256m</argLine>
        </configuration>
      </plugin>

      <plugin>
        <groupId>com.googlecode.maven-download-plugin</groupId>
        <artifactId>download-maven-plugin</artifactId>
        <version>${plugin.download.version}</version>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <version>${plugin.shade.version}</version>
        <configuration>
          <filters>
            <filter>
              <artifact>*:*</artifact>
              <excludes>
                <exclude>org/datanucleus/**</exclude>
                <exclude>META-INF/*.SF</exclude>
                <exclude>META-INF/*.DSA</exclude>
                <exclude>META-INF/*.RSA</exclude>
              </excludes>
            </filter>
          </filters>
          <transformers>
            <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
            <transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
              <resource>reference.conf</resource>
            </transformer>
          </transformers>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <!-- Deploy datanucleus jars to the interpreter/spark directory -->
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-dependency-plugin</artifactId>
        <executions>
          <execution>
            <id>copy-dependencies</id>
            <phase>package</phase>
            <goals>
              <goal>copy-dependencies</goal>
            </goals>
            <configuration>
              <outputDirectory>${project.build.directory}/../../interpreter/spark/dep</outputDirectory>
              <overWriteReleases>false</overWriteReleases>
              <overWriteSnapshots>false</overWriteSnapshots>
              <overWriteIfNewer>true</overWriteIfNewer>
              <includeGroupIds>org.datanucleus</includeGroupIds>
            </configuration>
          </execution>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>copy</goal>
            </goals>
            <configuration>
              <outputDirectory>${project.build.directory}/../../interpreter/spark/dep</outputDirectory>
              <overWriteReleases>false</overWriteReleases>
              <overWriteSnapshots>false</overWriteSnapshots>
              <overWriteIfNewer>true</overWriteIfNewer>
              <artifactItems>
                <artifactItem>
                  <groupId>${project.groupId}</groupId>
                  <artifactId>${project.artifactId}</artifactId>
                  <version>${project.version}</version>
                  <type>${project.packaging}</type>
                </artifactItem>
              </artifactItems>
            </configuration>
          </execution>
        </executions>
      </plugin>

      <!-- include pyspark by default -->
      <plugin>
        <groupId>com.googlecode.maven-download-plugin</groupId>
        <artifactId>download-maven-plugin</artifactId>
        <executions>
          <execution>
            <id>download-pyspark-files</id>
            <phase>validate</phase>
            <goals>
              <goal>wget</goal>
            </goals>
            <configuration>
              <readTimeOut>60000</readTimeOut>
              <retries>5</retries>
              <unpack>true</unpack>
              <url>${spark.src.download.url}</url>
              <outputDirectory>${project.build.directory}</outputDirectory>
            </configuration>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <artifactId>maven-clean-plugin</artifactId>
        <configuration>
          <filesets>
            <fileset>
              <directory>${basedir}/../python/build</directory>
            </fileset>
          </filesets>
        </configuration>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-antrun-plugin</artifactId>
        <executions>
          <execution>
            <id>zip-pyspark-files</id>
            <phase>generate-resources</phase>
            <goals>
              <goal>run</goal>
            </goals>
            <configuration>
              <target>
                <delete dir="../interpreter/spark/pyspark"/>
                <copy todir="../interpreter/spark/pyspark"
                      file="${project.build.directory}/${spark.archive}/python/lib/py4j-${spark.py4j.version}-src.zip"/>
                <zip destfile="${project.build.directory}/../../interpreter/spark/pyspark/pyspark.zip"
                     basedir="${project.build.directory}/${spark.archive}/python"
                     includes="pyspark/*.py,pyspark/**/*.py"/>
              </target>
            </configuration>
          </execution>
        </executions>
      </plugin>

      <!-- include sparkr by default -->
      <plugin>
        <groupId>com.googlecode.maven-download-plugin</groupId>
        <artifactId>download-maven-plugin</artifactId>
        <executions>
          <execution>
            <id>download-sparkr-files</id>
            <phase>validate</phase>
            <goals>
              <goal>wget</goal>
            </goals>
            <configuration>
              <readTimeOut>60000</readTimeOut>
              <retries>5</retries>
              <url>${spark.bin.download.url}</url>
              <unpack>true</unpack>
              <outputDirectory>${project.build.directory}</outputDirectory>
            </configuration>
          </execution>
        </executions>
      </plugin>
      <plugin>
        <artifactId>maven-resources-plugin</artifactId>
        <version>2.7</version>
        <executions>
          <execution>
            <id>copy-sparkr-files</id>
            <phase>generate-resources</phase>
            <goals>
              <goal>copy-resources</goal>
            </goals>
            <configuration>
              <outputDirectory>${project.build.directory}/../../interpreter/spark/R/lib</outputDirectory>
              <resources>
                <resource>
                  <directory>
                    ${project.build.directory}/spark-${spark.version}-bin-without-hadoop/R/lib
                  </directory>
                </resource>
              </resources>
            </configuration>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>
</project>
